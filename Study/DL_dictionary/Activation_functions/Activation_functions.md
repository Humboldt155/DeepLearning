## Activation functions


###1) ReLU

#####Rectivied Linear Unit (Выпрямитель)


ReLU имеет следующую формулу f(x) = max(0, x) 
и реализует простой пороговый переход в нуле.

![ReLU](imgs/ReLU.png)

Плюсы:

ReLU может быть реализован с 
помощью простого порогового преобразования 
матрицы активаций в нуле.
ReLU не подвержен насыщению.
Применение ReLU существенно повышает скорость 
сходимости стохастического градиентного спуска 
(в некоторых случаях до 6 раз ) 
по сравнению с сигмоидой и гиперболическим тангенсом. 
Считается, что это обусловлено линейным характером 
и отсутствием насыщения данной функции.

Минусы:

ReLU не всегда достаточно надежны 
и в процессе обучения могут выходить 
из строя («умирать»). 
Например, большой градиент, проходящий через ReLU, 
может привести к такому обновлению весов, 
что данный нейрон никогда больше не активируется. 
Если это произойдет, то, начиная с данного момента,
градиент, проходящий через этот нейрон, 
всегда будет равен нулю. Соответственно, 
данный нейрон будет необратимо выведен из строя. 
Например, при слишком большой скорости обучения 
(learning rate), может оказаться, что до 40% ReLU 
«мертвы» (то есть, никогда не активируются). 
Эта проблема решается посредством выбора надлежащей
скорости обучения.